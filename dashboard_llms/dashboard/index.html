<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>LLM Evaluation Dashboard</title>
    <link rel="stylesheet" href="style/style.css" />
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
  </head>
  <body>
    <div class="container">
      <h1>LLM Evaluation Dashboard</h1>

      <section class="section">
        <h2>General Results</h2>
        <p
          style="
            color: #666;
            font-style: italic;
            margin-bottom: 15px;
            font-size: 0.95rem;
          "
        >
          Select a Large Language Model (LLM) and a specific Prompt to load the
          individual evaluation metrics. This section serves as the starting
          point for analyzing the performance of a single model configuration.
        </p>
        <div class="controls">
          <label for="llm">LLM:</label>
          <select id="llm">
            <option value="gpt-3.5-turbo">GPT-3.5</option>
            <option value="gpt-4.1">GPT-4.1</option>
            <option value="gpt-4o-mini">GPT-4o-mini</option>
            <option value="gpt-4.1-mini">GPT-4.1-mini</option>
            <option value="gemini-2.5-flash">Gemini-2.5-flash</option>
          </select>

          <label for="prompt">Prompt:</label>
          <select id="prompt">
            <option value="prompt_1">Prompt 1</option>
            <option value="prompt_2">Prompt 2</option>
            <option value="prompt_4">Prompt 3</option>
            <option value="prompt_5">Prompt 4</option>
          </select>

          <button class="btn-primary" onclick="loadData()">Load Results</button>
        </div>

        <section class="section">
          <label>
            <input
              type="checkbox"
              id="toggleComparison"
              onchange="toggleComparison()"
            />
            Compare Models (Metrics per Class)
          </label>

          <div id="comparisonSection" style="display: none">
            <h2>Comparison</h2>
            <p
              style="
                color: #666;
                font-style: italic;
                margin-bottom: 15px;
                font-size: 0.95rem;
              "
            >
              Use this tool to compare the performance of two different models
              or prompts side-by-side. This allows for a direct assessment of
              relative strengths and weaknesses across specific metrics.
            </p>
            <div class="controls">
              <label for="llm1">Model 1:</label>
              <select id="llm1">
                <option value="gpt-3.5-turbo">GPT-3.5</option>
                <option value="gpt-4.1">GPT-4.1</option>
                <option value="gpt-4o-mini">GPT-4o-mini</option>
                <option value="gpt-4.1-mini">GPT-4.1-mini</option>
                <option value="gemini-2.5-flash">Gemini-2.5-flash</option>
              </select>

              <label for="llm2">Model 2:</label>
              <select id="llm2">
                <option value="gpt-3.5-turbo">GPT-3.5</option>
                <option value="gpt-4.1">GPT-4.1</option>
                <option value="gpt-4o-mini">GPT-4o-mini</option>
                <option value="gpt-4.1-mini">GPT-4.1-mini</option>
                <option value="gemini-2.5-flash">Gemini-2.5-flash</option>
              </select>

              <label for="promptCompare">Prompt:</label>
              <select id="promptCompare">
                <option value="prompt_1">Prompt 1</option>
                <option value="prompt_2">Prompt 2</option>
                <option value="prompt_4">Prompt 3</option>
                <option value="prompt_5">Prompt 4</option>
              </select>

              <label for="promptCompare2">Prompt:</label>
              <select id="promptCompare2">
                <option value="prompt_1">Prompt 1</option>
                <option value="prompt_2">Prompt 2</option>
                <option value="prompt_4">Prompt 3</option>
                <option value="prompt_5">Prompt 4</option>
              </select>

              <button class="btn-primary" onclick="compareSelected()">
                Compare
              </button>
            </div>
            <div id="comparisonResult"></div>
          </div>
        </section>

        <div
          class="collapsible-container"
          id="generalResultsContainer"
          style="display: none"
        >
          <div class="collapsible">
            <button type="button" class="collapsible-header">
              General Confusion Matrix
            </button>
            <div class="collapsible-content">
              <div class="collapsible-body">
                <p
                  style="
                    color: #666;
                    font-style: italic;
                    margin-bottom: 15px;
                    font-size: 0.95rem;
                  "
                >
                  This table displays the aggregate confusion matrix (True
                  Positives, False Positives, False Negatives, True Negatives)
                  for the selected model. It provides a high-level view of the
                  model's classification performance across all classes.
                </p>
                <div id="generalTable" style="display: none"></div>
                <div id="matrixTable" style="display: none"></div>
              </div>
            </div>
          </div>
          <div class="collapsible">
            <button type="button" class="collapsible-header">
              Metrics per Category
            </button>
            <div class="collapsible-content">
              <div class="collapsible-body">
                <p
                  style="
                    color: #666;
                    font-style: italic;
                    margin-bottom: 15px;
                    font-size: 0.95rem;
                  "
                >
                  View detailed performance metrics broken down by specific
                  categories. This helps identify if the model performs
                  significantly better in certain domain areas compared to
                  others.
                </p>
                <div id="categoryMetrics" style="display: none"></div>
              </div>
            </div>
          </div>
          <div class="collapsible">
            <button type="button" class="collapsible-header">
              Metrics per Class
            </button>
            <div class="collapsible-content">
              <div class="collapsible-body">
                <p
                  style="
                    color: #666;
                    font-style: italic;
                    margin-bottom: 15px;
                    font-size: 0.95rem;
                  "
                >
                  Analyze the precision, recall, and F1-score for each
                  individual class (e.g., Class 0, Class 1, NA). This highlights
                  the model's ability to correctly identify specific labels.
                </p>
                <div id="classMetrics" style="display: none"></div>
              </div>
            </div>
          </div>
          <div class="collapsible">
            <button type="button" class="collapsible-header">
              Metrics per Practice
            </button>
            <div class="collapsible-content">
              <div class="collapsible-body">
                <p
                  style="
                    color: #666;
                    font-style: italic;
                    margin-bottom: 15px;
                    font-size: 0.95rem;
                  "
                >
                  Examine the evaluation results at the granular level of
                  individual "Practices". This allows for pinpointing specific
                  instances where the model succeeds or fails.
                </p>
                <div id="practiceMetrics" style="display: none"></div>
              </div>
            </div>
          </div>

          <div class="collapsible">
            <button type="button" class="collapsible-header">
              Interactive Score Plots
            </button>
            <div class="collapsible-content">
              <div class="collapsible-body">
                <p
                  style="
                    color: #666;
                    font-style: italic;
                    margin-bottom: 15px;
                    font-size: 0.95rem;
                  "
                >
                  Visualize the scoring distribution using interactive plots.
                  Select a specific data file below to generate a graphical
                  representation of the model's trustworthiness scores.
                </p>
                <div class="controls">
                  <label for="fileSelect">Choose FileName:</label>
                  <select id="fileSelect"></select>

                  <button class="btn-primary" onclick="loadScoreGraphic()">
                    Load Results
                  </button>
                </div>

                <div id="mainPlot"></div>

                <h3 id="detailTitle" style="display: none">Method Details</h3>
                <div id="detailTable"></div>
              </div>
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Prompts Heatmap</h2>
        <p
          style="
            color: #666;
            font-style: italic;
            margin-bottom: 15px;
            font-size: 0.95rem;
          "
        >
          This section visualizes the performance of all models across different
          prompts simultaneously using heatmaps. Select a metric (e.g., F1
          Score) to generate a color-coded matrix, allowing for quick
          identification of the best-performing model-prompt combinations.
        </p>

        <div class="controls" style="margin-bottom: 20px">
          <label for="metricSelect" style="font-weight: bold"
            >Choose Metrics:</label
          >
          <select
            id="metricSelect"
            onchange="generateAllHeatmaps()"
            style="padding: 5px; margin-right: 10px"
          >
            <option value="f1">F1 Score (Macro)</option>
            <option value="precision">Precision (Macro)</option>
            <option value="recall">Recall (Macro)</option>
            <option value="accuracy">Accuracy (Macro)</option>
          </select>

          <button class="btn-primary" onclick="generateAllHeatmaps()">
            Generate Heatmaps
          </button>
        </div>

        <div id="globalHeatmapContainer"></div>

        <div
          id="heatmapLegend"
          style="margin-top: 20px; text-align: center; font-size: 14px"
        ></div>
      </section>
    </div>

    <script src="js/api.js"></script>
    <script src="js/render.js"></script>
    <script src="js/ui.js"></script>
    <script src="js/plots.js"></script>
    <script src="js/heatmap.js"></script>
    <script src="js/main.js"></script>
  </body>
</html>
